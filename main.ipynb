{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# <center> Project: NLP ENSAE \n",
    "## <center> Intents Classification for Neural Text Generation\n",
    "\n",
    "<center>Work done by : \n",
    "\n",
    "##### <center> Ali HAIDAR email : ali.haidar@polytechnique.edu\n",
    "##### <center> Fran√ßois Bertholom   email : \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from pytorch_pretrained_bert import BertModel\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import process\n",
    "import BertMultiClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset silicone (C:/Users/aliha/.cache/huggingface/datasets/silicone/dyda_da/1.0.0/af617406c94e3f78da85f7ea74ebfbd3f297a9665cb54adbae305b03bc4442a5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c3eebdc51349778db29857b156f058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('silicone','dyda_da')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(data=dataset['train'])\n",
    "val = pd.DataFrame(data=dataset['validation'])\n",
    "test = pd.DataFrame(data=dataset['test'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a classifier based on Bert that takes on input a uterance and gives the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train['Utterance']\n",
    "y_train = np.array(train['Label'])\n",
    "\n",
    "X_test = test['Utterance']\n",
    "y_test = np.array(test['Label'])\n",
    "\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "train_tokens_ids = process.tokenize(X_train)\n",
    "test_tokens_ids = process.tokenize(X_test)\n",
    "\n",
    "train_masks = process.mask(train_tokens_ids)\n",
    "test_mask = process.mask(test_tokens_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel_clf = BertMultiClassifier.BertMultiClassifier(n_classes)\\n\\n#train \\nBertMultiClassifier.train(model_clf , train_tokens_ids, train_masks, y_train,  BATCH_SIZE = 8, EPOCHS = 10)\\n\\n#test\\nBertMultiClassifier.test(model_clf, test_tokens_ids, test_masks, y_test, BATCH_SIZE = 8)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model_clf = BertMultiClassifier.BertMultiClassifier(n_classes)\n",
    "\n",
    "#train \n",
    "BertMultiClassifier.train(model_clf , train_tokens_ids, train_masks, y_train,  BATCH_SIZE = 8, EPOCHS = 10)\n",
    "\n",
    "#test\n",
    "BertMultiClassifier.test(model_clf, test_tokens_ids, test_masks, y_test, BATCH_SIZE = 8)\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a classifier based on Bert that takes on input a context and gives the label of each utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizeOfTheContext = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_context, test_context = process.context(train, test, sizeOfTheContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_context['Utterance']\n",
    "y_train = (train_context['Label'])\n",
    "\n",
    "X_test = test_context['Utterance']\n",
    "y_test = test_context['Label']\n",
    "\n",
    "y_train = np.array([np.array(i) for i in y_train])\n",
    "y_test =  np.array([np.array(i) for i in y_test])\n",
    "\n",
    "train_tokens_ids = process.tokenize(X_train)\n",
    "test_tokens_ids = process.tokenize(X_test)\n",
    "\n",
    "train_masks = process.mask(train_tokens_ids)\n",
    "test_mask = process.mask(test_tokens_ids)\n",
    "train_masks = np.array(train_masks)\n",
    "test_mask = np.array(test_mask)\n",
    "\n",
    "y_train_masks  = process.mask(y_train)\n",
    "y_test_masks  = process.mask(y_test)\n",
    "y_train_masks = np.array(y_train_masks)\n",
    "y_test_masks = np.array(y_test_masks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a classifier based on Bert as encoder and LSTM  as decoder that takes on input a text and gives the label of each utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embed_matrix():\n",
    "    bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "    bert_embeddings = list(bert.children())[0]\n",
    "    bert_word_embeddings = list(bert_embeddings.children())[0]\n",
    "    mat = bert_word_embeddings.weight.data.numpy()\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = get_bert_embed_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\n",
    "from keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "NUM_MODELS = 1\n",
    "BATCH_SIZE = 64\n",
    "LSTM_UNITS = 64\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "EPOCHS = 3\n",
    "\n",
    "\n",
    "def build_model(embedding_matrix, num_aux_targets):\n",
    "    words = Input(shape=(None,))\n",
    "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "    #x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "\n",
    "    hidden = concatenate([\n",
    "        GlobalMaxPooling1D()(x),\n",
    "        GlobalAveragePooling1D()(x),\n",
    "    ])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    result = Dense(1, activation='softmax')(hidden)\n",
    "    aux_result = Dense(num_aux_targets, activation='softmax')(hidden)\n",
    "    \n",
    "    model = Model(inputs=words, outputs=aux_result)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, None, 768)    23440896    ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " spatial_dropout1d_2 (SpatialDr  (None, None, 768)   0           ['embedding_2[0][0]']            \n",
      " opout1D)                                                                                         \n",
      "                                                                                                  \n",
      " bidirectional_4 (Bidirectional  (None, None, 256)   919552      ['spatial_dropout1d_2[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " bidirectional_5 (Bidirectional  (None, None, 256)   395264      ['bidirectional_4[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " global_max_pooling1d_2 (Global  (None, 256)         0           ['bidirectional_5[0][0]']        \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2 (Gl  (None, 256)         0           ['bidirectional_5[0][0]']        \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 512)          0           ['global_max_pooling1d_2[0][0]', \n",
      "                                                                  'global_average_pooling1d_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 512)          262656      ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 512)          0           ['concatenate_2[0][0]',          \n",
      "                                                                  'dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 512)          262656      ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 512)          0           ['add_4[0][0]',                  \n",
      "                                                                  'dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 1)            513         ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 4)            2052        ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 25,283,589\n",
      "Trainable params: 1,842,693\n",
      "Non-trainable params: 23,440,896\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model  = build_model(embedding_matrix, 4)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "      train_tokens_ids,\n",
    "      y_train,\n",
    "      batch_size=BATCH_SIZE,\n",
    "      epochs=EPOCHS,\n",
    "      verbose=2,\n",
    "  )        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_tokens_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\aliha\\OneDrive\\Desktop\\ENSAE\\NLP\\project\\main.ipynb Cell 20\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/aliha/OneDrive/Desktop/ENSAE/NLP/project/main.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39mpredict(test_tokens_ids, batch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mflatten()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_tokens_ids' is not defined"
     ]
    }
   ],
   "source": [
    "bert_predicted = np.argmax(model.predict(test_tokens_ids, batch_size=64),axis=1)\n",
    "acc = (np.sum(bert_predicted == y_test)/len(y_test)) *100\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
