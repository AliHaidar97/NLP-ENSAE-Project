{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# <center> Project: NLP ENSAE \n",
    "## <center> Intents Classification for Neural Text Generation\n",
    "\n",
    "<center>Work done by : \n",
    "\n",
    "##### <center> Ali HAIDAR email : ali.haidar@polytechnique.edu\n",
    "##### <center> Fran√ßois Bertholom   email : francois.bertholom@ensae.fr\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from pytorch_pretrained_bert import BertModel\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from IPython.display import clear_output\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate,Flatten\n",
    "from keras.layers import CuDNNLSTM, LSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, LSTM, Lambda,MaxPooling1D, AveragePooling1D,GRU, Reshape\n",
    "from keras.preprocessing import text, sequence\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "import keras.backend as K\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "import Process\n",
    "import Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-label One-target classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will evaluate and compare the effectiveness of a multi-label one target classifier, which takes an utterance as input and predicts the label for that utterance. \n",
    "\n",
    "To assess the performance of these models, we utilized the Silicone datasets provided by Hugging Face."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build three models based on Bert"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BertMLP1Layer: \n",
    "\n",
    "The model utilizes the embedding layer of a pre-trained BERT model. Following this, we incorporated a concatenation layer of GlobalMaxPooling1D and GlobalAveragePooling1D after the embedding. The resulting output was then passed through a single neural network layer.\n",
    "\n",
    "GlobalMaxPooling1D and GlobalAveragePooling1D are commonly used pooling operations in deep learning models, particularly for natural language processing (NLP) tasks.\n",
    "\n",
    "In the case of NLP, we often have input sequences of variable lengths. The pooling operations allow us to aggregate the information from these sequences into a fixed-length vector that can be passed to subsequent layers of the neural network.\n",
    "\n",
    "GlobalMaxPooling1D computes the maximum value from each feature dimension across the entire input sequence. This can be useful for capturing the most salient information in the input sequence.\n",
    "\n",
    "GlobalAveragePooling1D computes the average value from each feature dimension across the entire input sequence. This can be useful for capturing the overall distribution of information in the input sequence.\n",
    "\n",
    "By using both GlobalMaxPooling1D and GlobalAveragePooling1D in a concatenated layer, we can capture both the most salient information and the overall distribution of information in the input sequence, resulting in a more robust representation of the input that can improve model performance.\n",
    "\n",
    "The model employs the sparse_categorical_crossentropy as the loss function during the training phase. This loss function is commonly used for multi-class classification problems with integer labels.\n",
    "\n",
    "Furthermore, we evaluate the model's performance using the 0-1 accuracy metric. This metric measures the percentage of instances where the model correctly predicts the label for the input utterance. In other words, it calculates the ratio of the number of correctly predicted labels to the total number of labels, and then expresses this as a percentage. The 0-1 accuracy metric is a commonly used evaluation metric for multi-label classification problems, where each input can have multiple correct labels."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BertMLP2Layer:\n",
    "\n",
    "The main difference between this model and BertMLP1Layer is that we added an additional dense layer before the output layer. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BertGRU : \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"GRUModel.png\" alt=\"Bert GRU \" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_sparse_categorical_crossentropy(mask_value):\n",
    "    def loss_function(y_true, y_pred):\n",
    "        mask = K.cast(K.not_equal(y_true, mask_value), K.floatx())\n",
    "        masked_true = K.cast(mask * K.cast(y_true, K.floatx()), \"int32\")\n",
    "        loss = sparse_categorical_crossentropy(masked_true, y_pred)\n",
    "        masked_loss = loss * mask\n",
    "        return K.sum(masked_loss) / K.sum(mask)\n",
    "\n",
    "    return loss_function\n",
    "\n",
    "def generate_result(dataset, model, embedding_matrix, multi_target = 0):\n",
    "    \n",
    "    train = pd.DataFrame(data=dataset['train'])\n",
    "    val = pd.DataFrame(data=dataset['validation'])\n",
    "    test = pd.DataFrame(data=dataset['test'])\n",
    "    label = 'Label'\n",
    "    \n",
    "    if multi_target == 1 and 'Dialogue_ID' not in train.columns:\n",
    "        return 0\n",
    "    \n",
    "    if multi_target == 1:\n",
    "        train, val, test = Process.context(train.copy(), val.copy(), test.copy()) \n",
    "    \n",
    "    X_train = train['Utterance']\n",
    "    y_train = np.array(train[label]) \n",
    "    \n",
    "    \n",
    "    X_val = val['Utterance']\n",
    "    y_val = np.array(val[label]) \n",
    "    \n",
    "    X_test = test['Utterance']\n",
    "    y_test = np.array(test[label]) \n",
    "    \n",
    "    if multi_target ==1:\n",
    "        y_train = np.array([[j for j in i] for i in y_train])\n",
    "        y_val = np.array([[j for j in i] for i in y_val])\n",
    "        y_test = np.array([[j for j in i] for i in y_test])\n",
    "    \n",
    "    if(multi_target == 1):\n",
    "        out = y_train.shape[1]\n",
    "    else:\n",
    "        out = 1\n",
    "        \n",
    "    n_classes = len(np.unique(y_train.reshape(-1)))\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    \n",
    "    train_tokens_ids = Process.tokenize(X_train, tokenizer)\n",
    "    val_tokens_ids = Process.tokenize(X_val, tokenizer)\n",
    "    test_tokens_ids = Process.tokenize(X_test, tokenizer)\n",
    "    \n",
    "    if(multi_target == 1):\n",
    "        y_train_masks = Process.mask(y_train)\n",
    "        y_val_masks = Process.mask(y_val)\n",
    "        y_test_masks = Process.mask(y_test)\n",
    "        \n",
    "        y_train_masks = np.array(y_train_masks)\n",
    "        y_val_masks = np.array(y_val_masks)\n",
    "        y_test_masks = np.array(y_test_masks)\n",
    "    \n",
    "    NUM_TRAIN_STEPS = (len(train_tokens_ids) // BATCH_SIZE) * EPOCHS\n",
    "    \n",
    "    model = model.build_model(embedding_matrix, n_classes, out)\n",
    "    \n",
    "    lr_scheduler = PolynomialDecay(initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=NUM_TRAIN_STEPS)\n",
    "    opt = Adam(learning_rate=lr_scheduler, clipnorm=1)\n",
    "    \n",
    "    if(multi_target == 1):\n",
    "        model.compile(loss=masked_sparse_categorical_crossentropy(-1), optimizer=opt)\n",
    "    else:\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer=opt)\n",
    "    \n",
    "    earlyStopping = EarlyStopping(monitor='val_loss', patience=6, verbose=1, mode='min')\n",
    "    mcp_save = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "   \n",
    "    model.fit(\n",
    "        train_tokens_ids,\n",
    "        y_train,\n",
    "        validation_data=(val_tokens_ids, y_val),\n",
    "        validation_batch_size=512,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        verbose=1,\n",
    "        callbacks=[earlyStopping, mcp_save]\n",
    "    )\n",
    "    \n",
    "    bert_predicted = np.argmax(model.predict(test_tokens_ids, batch_size=512), axis=-1).reshape(-1)\n",
    "    y_test = y_test.reshape(-1)\n",
    "    if(multi_target == 1):\n",
    "        bert_predicted = bert_predicted[y_test_masks.reshape(-1)]\n",
    "        y_test = y_test[y_test_masks.reshape(-1)]\n",
    "  \n",
    "    acc = (np.sum(bert_predicted == y_test) / len(y_test)) * 100\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "\n",
    "# Get the embedding matrix for BERT\n",
    "embedding_matrix = Models.get_bert_embed_matrix()\n",
    "\n",
    "# Initialize a results dataframe\n",
    "results = pd.DataFrame(columns=['model','dyda_da', 'dyda_e','maptask', 'meld_e', 'meld_s', 'mrda', 'oasis', 'sem'])\n",
    "\n",
    "# Initialize a list of models to evaluate\n",
    "models = [Models.BertMLP1Layer(), Models.BertMLP2Layers(), Models.BertGRU()]\n",
    "\n",
    "# Loop through the models and datasets to generate results\n",
    "for model in models:\n",
    "    # Create a list to store results for this model\n",
    "    res = [model.__class__.__name__]\n",
    "    \n",
    "    # Loop through the datasets\n",
    "    for d in ['dyda_da', 'dyda_e' ,'maptask', 'meld_e', 'meld_s', 'mrda', 'oasis', 'sem']:\n",
    "        # Load the dataset\n",
    "        dataset = load_dataset('silicone',d)\n",
    "        \n",
    "        # Generate accuracy for this dataset and model\n",
    "        acc = generate_result(dataset, model, embedding_matrix)\n",
    "        \n",
    "        # Print accuracy\n",
    "        print(\"Accuracy on \" + d + \" :\",acc)\n",
    "        \n",
    "        # Append accuracy to results list\n",
    "        res.append(acc)\n",
    "    \n",
    "    # Append results for this model to the overall results dataframe\n",
    "    results.loc[len(results)] = res\n",
    "    \n",
    "# Save results to a csv file\n",
    "results.to_csv(\"MultiLabelOneTarget.csv\",index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-label Multi-target classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we try to predict the labels from a context instead of a single utterance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "\n",
    "# Get the embedding matrix for BERT\n",
    "embedding_matrix = Models.get_bert_embed_matrix()\n",
    "\n",
    "# Initialize a results dataframe\n",
    "results = pd.DataFrame(columns=['model','dyda_da', 'dyda_e','maptask', 'meld_e', 'meld_s', 'mrda', 'oasis', 'sem'])\n",
    "\n",
    "# Initialize a list of models to evaluate\n",
    "models = [Models.BertMLP1Layer(), Models.BertMLP2Layers(), Models.BertGRU()]\n",
    "\n",
    "# Loop through the models and datasets to generate results\n",
    "for model in models:\n",
    "    # Create a list to store results for this model\n",
    "    res = [model.__class__.__name__]\n",
    "    \n",
    "    # Loop through the datasets\n",
    "    for d in ['dyda_da', 'dyda_e' ,'maptask', 'meld_e', 'meld_s', 'mrda', 'oasis', 'sem']:\n",
    "        # Load the dataset\n",
    "        dataset = load_dataset('silicone',d)\n",
    "        \n",
    "        # Generate accuracy for this dataset and model\n",
    "        acc = generate_result(dataset, model, embedding_matrix, multi_target = 1)\n",
    "        \n",
    "        # Print accuracy\n",
    "        print(\"Accuracy on \" + d + \" :\",acc)\n",
    "        \n",
    "        # Append accuracy to results list\n",
    "        res.append(acc)\n",
    "    \n",
    "    # Append results for this model to the overall results dataframe\n",
    "    results.loc[len(results)] = res\n",
    "    \n",
    "# Save results to a csv file\n",
    "results.to_csv(\"MultiLabelMultiTarget.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+-----------+----------+-----------+----------+----------+---------+---------+---------+\n",
      "|    | model          |   dyda_da |   dyda_e |   maptask |   meld_e |   meld_s |    mrda |   oasis |     sem |\n",
      "|----+----------------+-----------+----------+-----------+----------+----------+---------+---------+---------|\n",
      "|  0 | BertGRU        |   81.3178 |  84.7804 |   62.8542 |  62.1839 |  67.433  | 90.0194 | 66.7794 | 64.123  |\n",
      "|  1 | BertMLP2Layers |   78.9147 |  84.5349 |   59.6406 |  60      |  65.977  | 89.8255 | 58.728  | 57.631  |\n",
      "|  2 | BertMLP1Layer  |   76.3307 |  83.3204 |   53.1099 |  52.1839 |  60.6513 | 87.4337 | 50.6089 | 54.8975 |\n",
      "+----+----------------+-----------+----------+-----------+----------+----------+---------+---------+---------+\n"
     ]
    }
   ],
   "source": [
    "multiLabelOneTarget = pd.read_csv(\"results/MultiLabelOneTarget.csv\")\n",
    "print(tabulate(multiLabelOneTarget, headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+-----------+----------+-----------+----------+----------+---------+---------+---------+\n",
      "|    | model          |   dyda_da |   dyda_e |   maptask |   meld_e |   meld_s |    mrda |   oasis |     sem |\n",
      "|----+----------------+-----------+----------+-----------+----------+----------+---------+---------+---------|\n",
      "|  0 | BertGRU        |   54.031  |  76.5891 |         0 |  41.1494 |  41.2261 | 46.7292 |       0 | 26.9932 |\n",
      "|  1 | BertMLP2Layers |   39.199  |  70.9173 |         0 |  30.2682 |  30.3448 | 45.223  |       0 | 28.4738 |\n",
      "|  2 | BertMLP1Layer  |   34.4574 |  66.2145 |         0 |  30.2299 |  30.1533 | 46.8649 |       0 | 30.8656 |\n",
      "+----+----------------+-----------+----------+-----------+----------+----------+---------+---------+---------+\n"
     ]
    }
   ],
   "source": [
    "multiLabelMultiTarget = pd.read_csv(\"results/MultiLabelMultiTarget.csv\")\n",
    "print(tabulate(multiLabelMultiTarget, headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude that the performance of our models degrades for context classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
