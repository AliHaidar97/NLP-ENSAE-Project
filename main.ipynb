{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# <center> Project: NLP ENSAE \n",
    "## <center> Intents Classification for Neural Text Generation\n",
    "\n",
    "<center>Work done by : \n",
    "\n",
    "##### <center> Ali HAIDAR email : ali.haidar@polytechnique.edu\n",
    "##### <center> Fran√ßois Bertholom   email : \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from pytorch_pretrained_bert import BertModel\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate,Flatten\n",
    "from keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, LSTM\n",
    "from keras.preprocessing import text, sequence\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "\n",
    "\n",
    "import process\n",
    "import models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset silicone (C:/Users/aliha/.cache/huggingface/datasets/silicone/dyda_da/1.0.0/af617406c94e3f78da85f7ea74ebfbd3f297a9665cb54adbae305b03bc4442a5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c3eebdc51349778db29857b156f058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('silicone','dyda_da')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(data=dataset['train'])\n",
    "val = pd.DataFrame(data=dataset['validation'])\n",
    "test = pd.DataFrame(data=dataset['test'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a classifier based on Bert that takes on input a context and gives the label of each utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "sizeOfTheContext = 5\n",
    "\n",
    "train_context, test_context = process.context(train, test, sizeOfTheContext)\n",
    "\n",
    "X_train = train_context['Utterance']\n",
    "y_train = (train_context['Label'])\n",
    "\n",
    "X_test = test_context['Utterance']\n",
    "y_test = test_context['Label']\n",
    "\n",
    "y_train = np.array([np.array(i) for i in y_train])\n",
    "y_test =  np.array([np.array(i) for i in y_test])\n",
    "\n",
    "train_tokens_ids = process.tokenize(X_train)\n",
    "test_tokens_ids = process.tokenize(X_test)\n",
    "\n",
    "train_masks = process.mask(train_tokens_ids)\n",
    "test_mask = process.mask(test_tokens_ids)\n",
    "train_masks = np.array(train_masks)\n",
    "test_mask = np.array(test_mask)\n",
    "\n",
    "y_train_masks  = process.mask(y_train)\n",
    "y_test_masks  = process.mask(y_test)\n",
    "y_train_masks = np.array(y_train_masks)\n",
    "y_test_masks = np.array(y_test_masks)\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a classifier based on Bert that takes on input a uterance and gives the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embed_matrix():\n",
    "    bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "    bert_embeddings = list(bert.children())[0]\n",
    "    bert_word_embeddings = list(bert_embeddings.children())[0]\n",
    "    mat = bert_word_embeddings.weight.data.numpy()\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_result(dataset,model):\n",
    "    \n",
    "    train = pd.DataFrame(data=dataset['train'])\n",
    "    val = pd.DataFrame(data=dataset['validation'])\n",
    "    test = pd.DataFrame(data=dataset['test'])\n",
    "    label = 'Label'\n",
    "\n",
    "    train = train.dropna()\n",
    "    val = val.dropna()\n",
    "    X_train = train['Utterance']\n",
    "    y_train = np.array(train[label])\n",
    "\n",
    "    X_val = val['Utterance']\n",
    "    y_val = np.array(val[label])\n",
    "\n",
    "    X_test = test['Utterance']\n",
    "    y_test = np.array(test[label])\n",
    "\n",
    "    n_classes = len(np.unique(y_train))\n",
    "   \n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)\n",
    "\n",
    "    train_tokens_ids = process.tokenize(X_train, tokenizer)\n",
    "    val_tokens_ids = process.tokenize(X_val, tokenizer)\n",
    "    test_tokens_ids = process.tokenize(X_test, tokenizer)\n",
    "    \n",
    "    embedding_matrix = get_bert_embed_matrix()\n",
    "    model = model.build_model(embedding_matrix, n_classes)\n",
    "\n",
    "    earlyStopping = EarlyStopping(monitor='val_loss', patience=6, verbose=1, mode='min')\n",
    "    mcp_save = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "    \n",
    "    NUM_TRAIN_STEPS = (len(train_tokens_ids)//BATCH_SIZE) * EPOCHS\n",
    "\n",
    "    lr_scheduler = PolynomialDecay(initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps= NUM_TRAIN_STEPS)\n",
    "    opt = Adam(learning_rate=lr_scheduler,clipnorm=1)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=opt)\n",
    "    print(model.summary())\n",
    "    model.fit(\n",
    "      train_tokens_ids,\n",
    "      y_train,\n",
    "\n",
    "      validation_data = (val_tokens_ids,y_val),\n",
    "      validation_batch_size = 512,\n",
    "      batch_size=BATCH_SIZE,\n",
    "      epochs=EPOCHS,\n",
    "      verbose=1,\n",
    "      callbacks=[earlyStopping, mcp_save]\n",
    "    )\n",
    "\n",
    "    bert_predicted = np.argmax(model.predict(test_tokens_ids, batch_size=128),axis=1)\n",
    "    acc = (np.sum(bert_predicted == y_test)/len(y_test)) *100\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "\n",
    "embedding_matrix = get_bert_embed_matrix()\n",
    "results = pd.DataFrame(columns=['model','dyda_da', 'dyda_e','maptask', 'meld_e', 'meld_s', 'mrda', 'oasis', 'sem', 'swda','iemocap'])\n",
    "models = [models.BertMLP1Layer(), models.BertMLP2Layers(), models.BertLstm(), models.BertDoubleLstm()]\n",
    "for model in models:\n",
    "  res = [model.__class__.__name__]\n",
    "  for d in ['dyda_da', 'dyda_e' ,'maptask', 'meld_e', 'meld_s', 'mrda', 'oasis', 'sem', 'swda','iemocap']:\n",
    "      dataset = load_dataset('silicone',d)\n",
    "      acc = generate_result(dataset, model, embedding_matrix)\n",
    "      print(\"Accuracy on \" + d + \" :\",acc)\n",
    "      res.append(acc)\n",
    "  results.loc[len(results)] = res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
